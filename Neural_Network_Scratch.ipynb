{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39009f76-98a7-47ca-b3ca-47bf28966f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea20ee71-dd06-4a0d-890c-68de3779f33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Model Demonstration\n",
      "================================================================================\n",
      "\n",
      "1. DATASET GENERATION AND PREPROCESSING\n",
      "--------------------------------------------------\n",
      "Dataset shape: (2000, 20)\n",
      "Number of classes: 3\n",
      "Feature statistics:\n",
      " Mean: 0.0831\n",
      " Std: 3.8041\n",
      " Min: -25.1647\n",
      " Max: 31.8324\n",
      "\n",
      "After standardization:\n",
      " Mean: -0.0000\n",
      " Std: 1.0000\n",
      "\n",
      "Training set: (1600, 20)\n",
      "Test set: (400, 20)\n",
      "\n",
      "2. NEURAL NETWORK ARCHITECTURE DESIGN\n",
      "--------------------------------------------------\n",
      "Network architecture: [20, 64, 32, 16, 3]\n",
      "Layer 1: 20 → 64 | Parameters: 1344\n",
      "Total parameters: 1344\n",
      "\n",
      "Mathematical hyperparameter selection:\n",
      "Learning rate: 0.001 (balanced convergence speed)\n",
      "Batch size: 64 (computational efficiency vs. gradient accuracy)\n",
      "Regularization: 0.001 (prevent overfitting)\n",
      "Optimizer: Adam (adaptive learning rates)\n",
      "\n",
      "3. MODEL TRAINING AND Optimization\n",
      "--------------------------------------------------\n",
      "Layer 2: 64 → 32 | Parameters: 2080\n",
      "Total parameters: 3424\n",
      "\n",
      "Mathematical hyperparameter selection:\n",
      "Learning rate: 0.001 (balanced convergence speed)\n",
      "Batch size: 64 (computational efficiency vs. gradient accuracy)\n",
      "Regularization: 0.001 (prevent overfitting)\n",
      "Optimizer: Adam (adaptive learning rates)\n",
      "\n",
      "3. MODEL TRAINING AND Optimization\n",
      "--------------------------------------------------\n",
      "Layer 3: 32 → 16 | Parameters: 528\n",
      "Total parameters: 3952\n",
      "\n",
      "Mathematical hyperparameter selection:\n",
      "Learning rate: 0.001 (balanced convergence speed)\n",
      "Batch size: 64 (computational efficiency vs. gradient accuracy)\n",
      "Regularization: 0.001 (prevent overfitting)\n",
      "Optimizer: Adam (adaptive learning rates)\n",
      "\n",
      "3. MODEL TRAINING AND Optimization\n",
      "--------------------------------------------------\n",
      "Layer 4: 16 → 3 | Parameters: 51\n",
      "Total parameters: 4003\n",
      "\n",
      "Mathematical hyperparameter selection:\n",
      "Learning rate: 0.001 (balanced convergence speed)\n",
      "Batch size: 64 (computational efficiency vs. gradient accuracy)\n",
      "Regularization: 0.001 (prevent overfitting)\n",
      "Optimizer: Adam (adaptive learning rates)\n",
      "\n",
      "3. MODEL TRAINING AND Optimization\n",
      "--------------------------------------------------\n",
      "\n",
      "Training Adam + ReLU model:\n",
      "Starting Neural network Training...\n",
      "Architecture: [20, 64, 32, 16, 3]\n",
      "Optimizer: adam\n",
      "Activation: relu\n",
      "Learning rate: 0.001\n",
      "Regularization: 0.001\n",
      "--------------------------------------------------\n",
      "Epoch    0 | Loss: 1.068299 | Accuracy: 0.5863\n",
      "Epoch  100 | Loss: 0.076121 | Accuracy: 1.0000\n",
      "Epoch  200 | Loss: 0.059265 | Accuracy: 1.0000\n",
      "Epoch  300 | Loss: 0.054207 | Accuracy: 1.0000\n",
      "Epoch  400 | Loss: 0.050751 | Accuracy: 1.0000\n",
      "Training Ccompleted!\n",
      "Final Results - Train Accuracy: 1.0000, Test Accuracy: 0.9225\n",
      "\n",
      "4. Mathematical analysis and results\n",
      "--------------------------------------------------\n",
      "Model Performance Comparison:\n",
      "Model                Train Acc    Test Acc     Final Loss   Overfitting \n",
      "----------------------------------------------------------------------\n",
      "Adam + ReLU          1.0000       0.9225       0.054640     0.0775      \n",
      "\n",
      "Best performing model: Adam + ReLU\n",
      "Test accuracy: 0.9225\n",
      "\n",
      "5. Detailed Mathematical Analysis\n",
      "--------------------------------------------------\n",
      "Weight Distribution Analysis:\n",
      "\n",
      "Training SGD + Sigmoid model:\n",
      "Starting Neural network Training...\n",
      "Architecture: [20, 64, 32, 16, 3]\n",
      "Optimizer: sgd\n",
      "Activation: sigmoid\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.001\n",
      "--------------------------------------------------\n",
      "Epoch    0 | Loss: 1.278772 | Accuracy: 0.3350\n",
      "Epoch  100 | Loss: 1.131674 | Accuracy: 0.6288\n",
      "Epoch  200 | Loss: 1.103893 | Accuracy: 0.6631\n",
      "Epoch  300 | Loss: 0.996925 | Accuracy: 0.6694\n",
      "Epoch  400 | Loss: 0.839974 | Accuracy: 0.6881\n",
      "Training Ccompleted!\n",
      "Final Results - Train Accuracy: 0.7056, Test Accuracy: 0.6750\n",
      "\n",
      "4. Mathematical analysis and results\n",
      "--------------------------------------------------\n",
      "Model Performance Comparison:\n",
      "Model                Train Acc    Test Acc     Final Loss   Overfitting \n",
      "----------------------------------------------------------------------\n",
      "Adam + ReLU          1.0000       0.9225       0.054640     0.0775      \n",
      "SGD + Sigmoid        0.7056       0.6750       0.761892     0.0306      \n",
      "\n",
      "Best performing model: Adam + ReLU\n",
      "Test accuracy: 0.9225\n",
      "\n",
      "5. Detailed Mathematical Analysis\n",
      "--------------------------------------------------\n",
      "Weight Distribution Analysis:\n",
      "\n",
      "Training Momentum + Tanh model:\n",
      "Starting Neural network Training...\n",
      "Architecture: [20, 64, 32, 16, 3]\n",
      "Optimizer: momentum\n",
      "Activation: tanh\n",
      "Learning rate: 0.005\n",
      "Regularization: 0.001\n",
      "--------------------------------------------------\n",
      "Epoch    0 | Loss: 1.010669 | Accuracy: 0.6756\n",
      "Epoch  100 | Loss: 0.130399 | Accuracy: 0.9925\n",
      "Epoch  200 | Loss: 0.091626 | Accuracy: 1.0000\n",
      "Epoch  300 | Loss: 0.083298 | Accuracy: 1.0000\n",
      "Epoch  400 | Loss: 0.077850 | Accuracy: 1.0000\n",
      "Training Ccompleted!\n",
      "Final Results - Train Accuracy: 1.0000, Test Accuracy: 0.9000\n",
      "\n",
      "4. Mathematical analysis and results\n",
      "--------------------------------------------------\n",
      "Model Performance Comparison:\n",
      "Model                Train Acc    Test Acc     Final Loss   Overfitting \n",
      "----------------------------------------------------------------------\n",
      "Adam + ReLU          1.0000       0.9225       0.054640     0.0775      \n",
      "SGD + Sigmoid        0.7056       0.6750       0.761892     0.0306      \n",
      "Momentum + Tanh      1.0000       0.9000       0.073786     0.1000      \n",
      "\n",
      "Best performing model: Adam + ReLU\n",
      "Test accuracy: 0.9225\n",
      "\n",
      "5. Detailed Mathematical Analysis\n",
      "--------------------------------------------------\n",
      "Weight Distribution Analysis:\n",
      "Layer 1 weights: μ=0.0028, σ=0.1406, min=-0.4340, max=0.4827\n",
      "Layer 2 weights: μ=0.0035, σ=0.1087, min=-0.5766, max=0.4050\n",
      "Layer 3 weights: μ=0.0188, σ=0.2046, min=-0.5190, max=0.6494\n",
      "Layer 4 weights: μ=0.0013, σ=0.6657, min=-0.9762, max=1.1492\n",
      "\n",
      "Loss convergence analysis:\n",
      "Initial loss: 1.068299\n",
      "Final loss: 0.054640\n",
      "Loss reduction: 94.89%\n",
      "\n",
      "Computational Complexity Analysis:\n",
      "Forward pass complexity = O(4003)\n",
      "Backward pass complexity = O(4003)\n",
      "Memory complexity: O(4003) parameters\n"
     ]
    }
   ],
   "source": [
    "class Advance_Neural_Network:\n",
    "    def __init__(self, layers, learning_rate = 0.001, epochs = 100, regularization = 0.01,\n",
    "                optimizer = 'adam', activation = 'relu', batch_size = 16):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.regularization = regularization\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #Initializing weights and biases using Xavier initialiation\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            xavier = np.sqrt(2.0 / (layers[i] + layers[i + 1]))\n",
    "            weight_matrix = np.random.normal(0, xavier, (layers[i], layers[i + 1]))\n",
    "            bias_vector = np.zeros((1, layers[i + 1]))\n",
    "\n",
    "            self.weights.append(weight_matrix)\n",
    "            self.biases.append(bias_vector)\n",
    "\n",
    "        if self.optimizer == 'momentum':\n",
    "            self.velocity_w = [np.zeros_like(w) for w in self.weights] \n",
    "            self.velocity_b = [np.zeros_like(b) for b in self.biases] \n",
    "            self.momentum = 0.9\n",
    "\n",
    "        elif self.optimizer == 'adam':\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights] #First moment\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights] #Second moment\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.epsilon = 1e-8\n",
    "            self.t = 0 # Time setp\n",
    "\n",
    "\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def activation_function(self, z, derivative = False):\n",
    "        if self.activation == 'relu':\n",
    "            if derivative:\n",
    "                return np.where(z > 0, 1, 0)\n",
    "            return np.maximum(0, z)\n",
    "\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid = 1 / (1 + np.exp(-np.clip(z, -250, 250))) # Numerical stability\n",
    "            if derivative:\n",
    "                return sigmoid * (1 - sigmoid)\n",
    "            return sigmoid\n",
    "\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh = np.tanh(z)\n",
    "            if derivative:\n",
    "                return 1 - tanh**2\n",
    "            return tanh\n",
    "\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        z_values = []\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # linear transformation\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            z_values.append(z)\n",
    "\n",
    "            #Applying activationo fuction \n",
    "            if i == len(self.weights) - 1 and self.layers[-1] > 1:\n",
    "                a = self._softmax(z)\n",
    "            else:\n",
    "                a = self.activation_function(z)\n",
    "\n",
    "            activations.append(a)\n",
    "\n",
    "        return activations, z_values\n",
    "\n",
    "    def backward_propagation(self, X, y, activations, z_values):\n",
    "        m = X.shape[0]\n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "\n",
    "        # Output layer error\n",
    "        if self.layers[-1] == 1: # Regression\n",
    "            delta = activations[-1] - y.reshape(-1,1)\n",
    "        else: # Classification\n",
    "            delta = activations[-1] - y\n",
    "\n",
    "        # back propagation through all layers\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Gradient with respect to weights\n",
    "            grad_w = np.dot(activations[i].T, delta) / m\n",
    "\n",
    "            # adding L2 regularizations\n",
    "            grad_w += self.regularization * self.weights[i]\n",
    "\n",
    "            # Gradient with respect to bias\n",
    "            grad_b = np.mean(delta, axis = 0, keepdims = True)\n",
    "\n",
    "            gradients_w.insert(0, grad_w)\n",
    "            gradients_b.insert(0, grad_b)\n",
    "\n",
    "            # error to previous layer\n",
    "            if i > 0:\n",
    "                # applyinnh the activation derivative\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_function(z_values[i-1], derivative=True)\n",
    "\n",
    "\n",
    "        return gradients_w, gradients_b\n",
    "\n",
    "    def update_weights(self, gradients_w, gradients_b):\n",
    "        if self.optimizer == 'sgd':\n",
    "            # Standard gradient descent\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= self.learning_rate * gradients_w[i]\n",
    "                self.biases[i] -= self.learning_rate * gradients_b[i]\n",
    "\n",
    "        elif self.optimizer == 'momentum':\n",
    "            # momentum-based gradient descent\n",
    "            for i in range(len(self.weights)):\n",
    "                self.velocity_w[i] = self.momentum * self.velocity_w[i] + self.learning_rate * gradients_w[i]\n",
    "                self.velocity_b[i] = self.momentum * self.velocity_b[i] + self.learning_rate * gradients_b[i]\n",
    "\n",
    "                self.weights[i] -= self.velocity_w[i]\n",
    "                self.biases[i] -= self.velocity_b[i]\n",
    "\n",
    "        elif self.optimizer == 'adam':\n",
    "            # Adam optimizer with bias correction\n",
    "            self.t += 1\n",
    "\n",
    "            for i in range(len(self.weights)):\n",
    "                # update biased first moment estimate\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * gradients_w[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * gradients_b[i]\n",
    "\n",
    "                # update biased second raw moment estimate\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (gradients_w[i] **2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (gradients_b[i] **2)\n",
    "\n",
    "                # compute bias-corrected first moment estimate\n",
    "                m_w_corrected = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "                m_b_corrected = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "                # compute bias-corrected second raw moment estimate\n",
    "                v_w_corrected = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "                v_b_corrected = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                # Updating parameters\n",
    "                self.weights[i] -= self.learning_rate * m_w_corrected / (np.sqrt(v_w_corrected) + self.epsilon)\n",
    "                self.biases[i] -= self.learning_rate * m_b_corrected / (np.sqrt(v_b_corrected) + self.epsilon)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "\n",
    "        if self.layers[-1] == 1: #Regression\n",
    "            mse_loss = np.mean((y_true.reshape(-1,1) - y_pred) ** 2) / 2\n",
    "            \n",
    "        else: # Classification\n",
    "            y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "            log_likelihood = -np.sum(y_true * np.log(y_pred_clipped))\n",
    "            loss = log_likelihood / m\n",
    "            mse_loss = loss\n",
    "\n",
    "        # l2 regularization using\n",
    "        l2_penalty = 0\n",
    "        for weights in self.weights:\n",
    "            l2_penalty += np.sum(weights ** 2)\n",
    "        l2_penalty *= self.regularization / 2\n",
    "        return mse_loss + l2_penalty\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(f\"Starting Neural network Training...\")\n",
    "        print(f\"Architecture: {self.layers}\")\n",
    "        print(f\"Optimizer: {self.optimizer}\")\n",
    "        print(f\"Activation: {self.activation}\")\n",
    "        print(f\"Learning rate: {self.learning_rate}\")\n",
    "        print(f\"Regularization: {self.regularization}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffling for SGD\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            epoch_loss = 0\n",
    "            num_batchs = 0\n",
    "\n",
    "            # Batch trainig\n",
    "            for i in range(0, X.shape[0], self.batch_size):\n",
    "                batch_X = X_shuffled[i:i + self.batch_size]\n",
    "                batch_y = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                # Forward propagation\n",
    "                activations, z_values = self.forward_propagation(batch_X)\n",
    "\n",
    "                # Compute loss\n",
    "                batch_loss = self.compute_loss(batch_y, activations[-1])\n",
    "                epoch_loss += batch_loss\n",
    "                num_batchs += 1\n",
    "\n",
    "                # Backward propagation\n",
    "                gradients_w, gradients_b = self.backward_propagation(batch_X, batch_y, activations, z_values)\n",
    "\n",
    "                # Update weights\n",
    "                self.update_weights(gradients_w, gradients_b)\\\n",
    "\n",
    "            # Training matrices\n",
    "            avg_loss = epoch_loss / num_batchs\n",
    "            self.loss_history.append(avg_loss)\n",
    "\n",
    "            # Classification accuracy\n",
    "            if self.layers[-1] > 1:\n",
    "                predictions = self.predict(X)\n",
    "                accuracy = np.mean(predictions == np.argmax(y, axis = 1))\n",
    "                self.accuracy_history.append(accuracy)\n",
    "\n",
    "            # Progess\n",
    "            if epoch % 100 == 0:\n",
    "                if self.layers[-1] > 1:\n",
    "                    print(f\"Epoch {epoch:4d} | Loss: {avg_loss:.6f} | Accuracy: {accuracy:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:4d} | Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        print(\"Training Ccompleted!\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "\n",
    "        if self.layers[-1] == 1:\n",
    "            return activations[-1].flatten()\n",
    "        else:\n",
    "            return np.argmax(activations[-1], axis = 1)\n",
    "\n",
    "    def predict_probability(self, X):\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return activations[-1]\n",
    "\n",
    "def demonstrate_ml_model():\n",
    "    print(\"Model Demonstration\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    #  dataset for demonstration\n",
    "    print(\"\\n1. DATASET GENERATION AND PREPROCESSING\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    X, y = make_classification(n_samples=2000, n_features=20, n_informative=15,\n",
    "                               n_redundant=5, n_classes=3, n_clusters_per_class=2,\n",
    "                               random_state=42)\n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "    print(f\"Feature statistics:\")\n",
    "    print(f\" Mean: {np.mean(X):.4f}\")\n",
    "    print(f\" Std: {np.std(X):.4f}\")\n",
    "    print(f\" Min: {np.min(X):.4f}\")\n",
    "    print(f\" Max: {np.max(X):.4f}\")\n",
    "\n",
    "    #Standarization\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    print(f\"\\nAfter standardization:\")\n",
    "    print(f\" Mean: {np.mean(X_scaled):.4f}\")\n",
    "    print(f\" Std: {np.std(X_scaled):.4f}\")\n",
    "\n",
    "    y_onehot = np.eye(3)[y]\n",
    "\n",
    "    #Train test splitt\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.2,\n",
    "                                                        random_state=42, stratify=y)\n",
    "    print(f\"\\nTraining set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "    # Archi design\n",
    "    print(\"\\n2. Neural Network architecture design\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    architecture = [20, 64, 32, 16, 3]\n",
    "    print(f\"Network architecture: {architecture}\")\n",
    "\n",
    "    # parameters\n",
    "    total_params = 0\n",
    "    for i in range(len(architecture) - 1):\n",
    "        layer_params = architecture[i] * architecture[i + 1] + architecture[i + 1] # weights and bias\n",
    "        total_params += layer_params\n",
    "        print(f\"Layer {i+1}: {architecture[i]} → {architecture[i+1]} | Parameters: {layer_params}\")\n",
    "\n",
    "        print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "        # hypter params\n",
    "        print(f\"\\nMathematical hyperparameter selection:\")\n",
    "        print(f\"Learning rate: 0.001 (balanced convergence speed)\")\n",
    "        print(f\"Batch size: 64 (computational efficiency vs. gradient accuracy)\")\n",
    "        print(f\"Regularization: 0.001 (prevent overfitting)\")\n",
    "        print(f\"Optimizer: Adam (adaptive learning rates)\")\n",
    "\n",
    "        # Creating and training multiple models for comparison\n",
    "        print(\"\\n3. Model Training and Optimization\")\n",
    "        print(\"-\" * 50)\n",
    "        models = {\n",
    "            'Adam + ReLU': Advance_Neural_Network(\n",
    "                layers = architecture, learning_rate=0.001,epochs=500, regularization=0.001,\n",
    "                optimizer='adam', activation='relu', batch_size=64), \n",
    "            \n",
    "            'SGD + Sigmoid': Advance_Neural_Network(\n",
    "                layers = architecture, learning_rate=0.01,epochs=500,regularization=0.001,\n",
    "                optimizer='sgd', activation='sigmoid',batch_size=64),\n",
    "            \n",
    "            'Momentum + Tanh': Advance_Neural_Network(\n",
    "                layers = architecture, learning_rate=0.005, epochs=500,regularization=0.001,\n",
    "                optimizer='momentum', activation='tanh', batch_size=64)\n",
    "        }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} model:\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        train_predictions = model.predict(X_train)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        train_accuracy = np.mean(train_predictions == np.argmax(y_train, axis=1))\n",
    "        test_accuracy = np.mean(test_predictions == np.argmax(y_test, axis=1))\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'final_loss': model.loss_history[-1]\n",
    "        }\n",
    "\n",
    "        print(f\"Final Results - Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        # Mathematical analysis of results\n",
    "        print(\"\\n4. Mathematical analysis and results\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        print(\"Model Performance Comparison:\")\n",
    "        print(f\"{'Model':<20} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12} {'Overfitting':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for name, result in results.items():\n",
    "            overfitting = result['train_accuracy'] - result['test_accuracy']\n",
    "            print(f\"{name:<20} {result['train_accuracy']:<12.4f} {result['test_accuracy']:<12.4f} \"\n",
    "                  f\"{result['final_loss']:<12.6f} {overfitting:<12.4f}\")\n",
    "        \n",
    "        # Finding best model\n",
    "        best_model_name = max(results.keys(), key=lambda k: results[k]['test_accuracy'])\n",
    "        best_model = results[best_model_name]['model']\n",
    "        \n",
    "        print(f\"\\nBest performing model: {best_model_name}\")\n",
    "        print(f\"Test accuracy: {results[best_model_name]['test_accuracy']:.4f}\")\n",
    "        \n",
    "        # Detailed mathematical analysis\n",
    "        print(\"\\n5. Detailed Mathematical Analysis\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Analyze weight distributions\n",
    "        print(\"Weight Distribution Analysis:\")\n",
    "        \n",
    "    for i, weight_matrix in enumerate(best_model.weights):\n",
    "        weight_mean = np.mean(weight_matrix)\n",
    "        weight_std = np.std(weight_matrix)\n",
    "        weight_min = np.min(weight_matrix)\n",
    "        weight_max = np.max(weight_matrix)\n",
    "    \n",
    "        print(f\"Layer {i+1} weights: μ={weight_mean:.4f}, σ={weight_std:.4f}, \"\n",
    "              f\"min={weight_min:.4f}, max={weight_max:.4f}\")\n",
    "    \n",
    "    # Gradient analysis (approximate)\n",
    "    print(f\"\\nLoss convergence analysis:\")\n",
    "    initial_loss = best_model.loss_history[0]\n",
    "    final_loss = best_model.loss_history[-1]\n",
    "    loss_reduction = (initial_loss - final_loss) / initial_loss * 100\n",
    "    \n",
    "    print(f\"Initial loss: {initial_loss:.6f}\")\n",
    "    print(f\"Final loss: {final_loss:.6f}\")\n",
    "    print(f\"Loss reduction: {loss_reduction:.2f}%\")\n",
    "    \n",
    "    # Mathematical complexity analysis\n",
    "    print(f\"\\nComputational Complexity Analysis:\")\n",
    "    print(f\"Forward pass complexity = O({total_params})\")\n",
    "    print(f\"Backward pass complexity = O({total_params})\")\n",
    "    print(f\"Memory complexity: O({total_params}) parameters\")\n",
    "        \n",
    "    return best_model, results\n",
    "            \n",
    "        # Execute the comprehensive demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    model, results = demonstrate_ml_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83e2f1-67ad-4270-b37f-9cd79d95952d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
